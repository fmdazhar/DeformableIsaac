
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/runs
Actor MLP: Actor(
  (priv_encoder): Identity()
  (history_encoder): StateHistoryEncoder(
    (activation_fn): ELU(alpha=1.0)
    (encoder): Sequential(
      (0): Linear(in_features=48, out_features=30, bias=True)
      (1): ELU(alpha=1.0)
    )
    (conv_layers): Sequential(
      (0): Conv1d(30, 20, kernel_size=(4,), stride=(2,))
      (1): ELU(alpha=1.0)
      (2): Conv1d(20, 10, kernel_size=(2,), stride=(1,))
      (3): ELU(alpha=1.0)
      (4): Flatten(start_dim=1, end_dim=-1)
    )
    (linear_output): Sequential(
      (0): Linear(in_features=30, out_features=28, bias=True)
      (1): ELU(alpha=1.0)
    )
  )
  (actor_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (actor_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=12, bias=True)
    (5): Tanh()
  )
)
Critic MLP: Critic(
  (critic_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (critic_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
ActorCritic                              12
â”œâ”€Actor: 1-1                             --
â”‚    â””â”€Identity: 2-1                     --
â”‚    â””â”€StateHistoryEncoder: 2-2          --
â”‚    â”‚    â””â”€ELU: 3-1                     --
â”‚    â”‚    â””â”€Sequential: 3-2              1,470
â”‚    â”‚    â””â”€Sequential: 3-3              2,830
â”‚    â”‚    â””â”€Sequential: 3-4              868
â”‚    â””â”€Sequential: 2-3                   --
â”‚    â”‚    â””â”€Linear: 3-5                  9,856
â”‚    â”‚    â””â”€ELU: 3-6                     --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-7                  16,512
â”‚    â”‚    â””â”€ELU: 3-8                     --
â”‚    â”‚    â””â”€Linear: 3-9                  16,512
â”‚    â”‚    â””â”€ELU: 3-10                    --
â”‚    â”‚    â””â”€Linear: 3-11                 1,548
â”‚    â”‚    â””â”€Tanh: 3-12                   --
â”œâ”€Critic: 1-2                            --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-13                 9,856
â”‚    â”‚    â””â”€ELU: 3-14                    --
â”‚    â””â”€Sequential: 2-6                   --
â”‚    â”‚    â””â”€Linear: 3-15                 16,512
â”‚    â”‚    â””â”€ELU: 3-16                    --
â”‚    â”‚    â””â”€Linear: 3-17                 16,512
â”‚    â”‚    â””â”€ELU: 3-18                    --
â”‚    â”‚    â””â”€Linear: 3-19                 129
=================================================================
Total params: 92,617
Trainable params: 92,617
Non-trainable params: 0
=================================================================
[2025-03-20 17:38:12] Running RL reset
################################################################################
                     [1m Learning iteration 0/1000000 
                       Computation: 25 steps/s (collection: 1.400s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: 0.0000
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.93
     action noise std distribution: [0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40
                    Iteration time: 1.56s
                        Total time: 1.56s
                               ETA: 1562695.7s
################################################################################
                     [1m Learning iteration 1/1000000 
                       Computation: 36 steps/s (collection: 0.994s, learning 0.099s)
               Value function loss: 0.0023
                    Surrogate loss: 14.1810
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7993208765983582, 1.0013532638549805, 1.00283944606781, 0.803728461265564, 1.0020198822021484, 1.0026124715805054, 0.803173303604126, 1.002321720123291, 1.002862811088562, 0.8020820617675781, 1.001436710357666, 0.9998897314071655]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80
                    Iteration time: 1.09s
                        Total time: 2.66s
                               ETA: 1328150.1s
[34m[1mwandb[39m[22m: [33mWARNING[39m Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'
################################################################################
                     [1m Learning iteration 2/1000000 
                       Computation: 37 steps/s (collection: 0.972s, learning 0.091s)
               Value function loss: 0.0030
                    Surrogate loss: 38.6255
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7983406782150269, 1.0000416040420532, 1.0053927898406982, 0.8076226711273193, 1.0031315088272095, 1.0059878826141357, 0.8049148917198181, 1.0012743473052979, 1.0044444799423218, 0.8042357563972473, 1.0041382312774658, 1.001532793045044]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120
                    Iteration time: 1.06s
                        Total time: 3.72s
                               ETA: 1239952.8s
################################################################################
                     [1m Learning iteration 3/1000000 
                       Computation: 36 steps/s (collection: 1.016s, learning 0.090s)
               Value function loss: 0.1803
                    Surrogate loss: 30.6919
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4227
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7995023131370544, 1.0004273653030396, 1.0052549839019775, 0.8115960359573364, 1.0020369291305542, 1.006239891052246, 0.8064581751823425, 1.0018284320831299, 1.005419373512268, 0.8033871054649353, 1.0071371793746948, 1.0018624067306519]
                       Mean reward: -0.98
               Mean episode length: 148.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0388
 Mean episode rew_tracking_ang_vel: 0.0111
        Mean episode rew_lin_vel_z: -0.0305
       Mean episode rew_ang_vel_xy: -0.0043
          Mean episode rew_torques: -0.0474
          Mean episode rew_dof_acc: -0.0008
    Mean episode rew_feet_air_time: -0.0071
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0656
   Mean episode rew_dof_pos_limits: -0.0007
        Mean episode terrain_level: 0.3250
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160
                    Iteration time: 1.11s
                        Total time: 4.83s
                               ETA: 1206490.2s
################################################################################
                     [1m Learning iteration 4/1000000 
                       Computation: 34 steps/s (collection: 1.020s, learning 0.130s)
               Value function loss: 2.1294
                    Surrogate loss: 247.0393
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.2947
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8008855581283569, 1.0027872323989868, 1.0061978101730347, 0.8162765502929688, 1.0032563209533691, 1.0082123279571533, 0.810754656791687, 1.002988338470459, 1.008800983428955, 0.8045550584793091, 1.0106134414672852, 1.0031427145004272]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.02
 Mean episode rew_tracking_lin_vel: 0.1312
 Mean episode rew_tracking_ang_vel: 0.0394
        Mean episode rew_lin_vel_z: -0.1108
       Mean episode rew_ang_vel_xy: -0.0141
          Mean episode rew_torques: -0.1373
          Mean episode rew_dof_acc: -0.0028
    Mean episode rew_feet_air_time: -0.0202
        Mean episode rew_collision: -0.0330
      Mean episode rew_action_rate: -0.1918
   Mean episode rew_dof_pos_limits: -0.0020
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 200
                    Iteration time: 1.15s
                        Total time: 5.98s
                               ETA: 1195281.3s
################################################################################
                     [1m Learning iteration 5/1000000 
                       Computation: 33 steps/s (collection: 1.088s, learning 0.089s)
               Value function loss: 0.2152
                    Surrogate loss: 199.6859
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.3989
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8024036884307861, 1.003431797027588, 1.0084542036056519, 0.8169198632240295, 1.0052894353866577, 1.0107721090316772, 0.8134571313858032, 1.004378080368042, 1.0127204656600952, 0.8043176531791687, 1.011459231376648, 1.0042500495910645]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.02
 Mean episode rew_tracking_lin_vel: 0.2761
 Mean episode rew_tracking_ang_vel: 0.1039
        Mean episode rew_lin_vel_z: -0.3212
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_torques: -0.0323
          Mean episode rew_dof_acc: -0.0053
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -0.4400
      Mean episode rew_action_rate: -0.0690
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 240
                    Iteration time: 1.18s
                        Total time: 7.15s
                               ETA: 1192250.1s
################################################################################
                     [1m Learning iteration 6/1000000 
                       Computation: 33 steps/s (collection: 1.093s, learning 0.088s)
               Value function loss: 0.0922
                    Surrogate loss: 35.3256
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.4193
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8053492307662964, 1.0039246082305908, 1.0106996297836304, 0.818119466304779, 1.006569743156433, 1.0104516744613647, 0.8162829875946045, 1.0069117546081543, 1.0167291164398193, 0.8037411570549011, 1.0134575366973877, 1.0053393840789795]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.2761
 Mean episode rew_tracking_ang_vel: 0.1039
        Mean episode rew_lin_vel_z: -0.3212
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_torques: -0.0323
          Mean episode rew_dof_acc: -0.0053
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -0.4400
      Mean episode rew_action_rate: -0.0690
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 280
                    Iteration time: 1.18s
                        Total time: 8.33s
                               ETA: 1190700.8s
################################################################################
                     [1m Learning iteration 7/1000000 
                       Computation: 34 steps/s (collection: 1.083s, learning 0.090s)
               Value function loss: 0.1045
                    Surrogate loss: 156.8579
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.4369
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8092195987701416, 1.0053595304489136, 1.013448715209961, 0.8203052282333374, 1.0082991123199463, 1.0112534761428833, 0.82017582654953, 1.008950114250183, 1.0207247734069824, 0.8053151965141296, 1.0164223909378052, 1.0078262090682983]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.00
 Mean episode rew_tracking_lin_vel: 0.2761
 Mean episode rew_tracking_ang_vel: 0.1039
        Mean episode rew_lin_vel_z: -0.3212
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_torques: -0.0323
          Mean episode rew_dof_acc: -0.0053
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -0.4400
      Mean episode rew_action_rate: -0.0690
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 320
                    Iteration time: 1.17s
                        Total time: 9.51s
                               ETA: 1188425.8s
################################################################################
                     [1m Learning iteration 8/1000000 
                       Computation: 34 steps/s (collection: 1.072s, learning 0.093s)
               Value function loss: 13.4971
                    Surrogate loss: 14.7906
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.1211
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8118024468421936, 1.0077800750732422, 1.0154322385787964, 0.8207824230194092, 1.0101181268692017, 1.0132231712341309, 0.8227232694625854, 1.009847640991211, 1.023617148399353, 0.8080551624298096, 1.0158324241638184, 1.0075047016143799]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.1798
 Mean episode rew_tracking_ang_vel: 0.1303
        Mean episode rew_lin_vel_z: -0.2914
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_torques: -0.0733
          Mean episode rew_dof_acc: -0.0046
    Mean episode rew_feet_air_time: -0.0030
        Mean episode rew_collision: -0.9090
      Mean episode rew_action_rate: -0.1196
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360
                    Iteration time: 1.17s
                        Total time: 10.67s
                               ETA: 1185880.5s
################################################################################
                     [1m Learning iteration 9/1000000 
                       Computation: 35 steps/s (collection: 1.053s, learning 0.090s)
               Value function loss: 190.2744
                    Surrogate loss: 159.4751
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4514
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8144801259040833, 1.0112762451171875, 1.0186890363693237, 0.8222103714942932, 1.0128408670425415, 1.014978289604187, 0.8262513875961304, 1.0117356777191162, 1.0261675119400024, 0.811477780342102, 1.0158309936523438, 1.0090667009353638]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0009
 Mean episode rew_tracking_ang_vel: 0.1792
        Mean episode rew_lin_vel_z: -0.2360
       Mean episode rew_ang_vel_xy: -0.0075
          Mean episode rew_torques: -0.1495
          Mean episode rew_dof_acc: -0.0033
    Mean episode rew_feet_air_time: -0.0085
        Mean episode rew_collision: -1.7800
      Mean episode rew_action_rate: -0.2136
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 400
                    Iteration time: 1.14s
                        Total time: 11.82s
                               ETA: 1181535.0s
################################################################################
                    [1m Learning iteration 10/1000000 
                       Computation: 37 steps/s (collection: 0.982s, learning 0.089s)
               Value function loss: 437.9857
                    Surrogate loss: 106.9117
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4632
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8176791667938232, 1.0135585069656372, 1.0224236249923706, 0.8228294849395752, 1.0144615173339844, 1.0141890048980713, 0.8294875621795654, 1.013823390007019, 1.0291317701339722, 0.8145076036453247, 1.0168325901031494, 1.0123320817947388]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0009
 Mean episode rew_tracking_ang_vel: 0.1792
        Mean episode rew_lin_vel_z: -0.2360
       Mean episode rew_ang_vel_xy: -0.0075
          Mean episode rew_torques: -0.1495
          Mean episode rew_dof_acc: -0.0033
    Mean episode rew_feet_air_time: -0.0085
        Mean episode rew_collision: -1.7800
      Mean episode rew_action_rate: -0.2136
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 440
                    Iteration time: 1.07s
                        Total time: 12.89s
                               ETA: 1171530.3s
################################################################################
                    [1m Learning iteration 11/1000000 
                       Computation: 35 steps/s (collection: 1.029s, learning 0.090s)
               Value function loss: 1146.4254
                    Surrogate loss: 9.1796
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4666
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8209311962127686, 1.0156632661819458, 1.0242938995361328, 0.8233876824378967, 1.0144137144088745, 1.0143771171569824, 0.8322690725326538, 1.0162923336029053, 1.0321930646896362, 0.8160780072212219, 1.016311526298523, 1.0152194499969482]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.00
 Mean episode rew_tracking_lin_vel: 0.0009
 Mean episode rew_tracking_ang_vel: 0.1792
        Mean episode rew_lin_vel_z: -0.2360
       Mean episode rew_ang_vel_xy: -0.0075
          Mean episode rew_torques: -0.1495
          Mean episode rew_dof_acc: -0.0033
    Mean episode rew_feet_air_time: -0.0085
        Mean episode rew_collision: -1.7800
      Mean episode rew_action_rate: -0.2136
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 480
                    Iteration time: 1.12s
                        Total time: 14.01s
                               ETA: 1167188.3s
################################################################################
                    [1m Learning iteration 12/1000000 
                       Computation: 36 steps/s (collection: 0.996s, learning 0.090s)
               Value function loss: 1304.9025
                    Surrogate loss: 49.4217
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.4728
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8228293657302856, 1.017072319984436, 1.0259969234466553, 0.8240023255348206, 1.014121651649475, 1.0146476030349731, 0.8328494429588318, 1.0174630880355835, 1.0343356132507324, 0.8168600797653198, 1.0153089761734009, 1.0166574716567993]
                       Mean reward: -2.89
               Mean episode length: 124.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.4909
 Mean episode rew_tracking_ang_vel: 0.1269
        Mean episode rew_lin_vel_z: -0.2512
       Mean episode rew_ang_vel_xy: -0.0173
          Mean episode rew_torques: -0.1407
          Mean episode rew_dof_acc: -0.0044
    Mean episode rew_feet_air_time: -0.0032
        Mean episode rew_collision: -1.2967
      Mean episode rew_action_rate: -0.2234
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 520
                    Iteration time: 1.09s
                        Total time: 15.09s
                               ETA: 1160903.7s
################################################################################
                    [1m Learning iteration 13/1000000 
                       Computation: 36 steps/s (collection: 1.016s, learning 0.089s)
               Value function loss: 4924.8770
                    Surrogate loss: 13.4870
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 4.1907
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.96
     action noise std distribution: [0.823077380657196, 1.0189942121505737, 1.0289595127105713, 0.8247441649436951, 1.0151773691177368, 1.0146030187606812, 0.8332831859588623, 1.018000841140747, 1.036676287651062, 0.8189519643783569, 1.0145694017410278, 1.0178790092468262]
                       Mean reward: -2.89
               Mean episode length: 124.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.7849
 Mean episode rew_tracking_ang_vel: 0.0954
        Mean episode rew_lin_vel_z: -0.2603
       Mean episode rew_ang_vel_xy: -0.0233
          Mean episode rew_torques: -0.1355
          Mean episode rew_dof_acc: -0.0051
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -1.0067
      Mean episode rew_action_rate: -0.2294
   Mean episode rew_dof_pos_limits: -0.0010
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 560
                    Iteration time: 1.10s
                        Total time: 16.20s
                               ETA: 1156878.6s
################################################################################
                    [1m Learning iteration 14/1000000 
                       Computation: 36 steps/s (collection: 1.008s, learning 0.096s)
               Value function loss: 8902.0695
                    Surrogate loss: 13.0167
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 4.1965
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.96
     action noise std distribution: [0.8238753080368042, 1.021021842956543, 1.0314056873321533, 0.8258734345436096, 1.0158424377441406, 1.0152537822723389, 0.8350818157196045, 1.0181044340133667, 1.0385456085205078, 0.8192251324653625, 1.0149574279785156, 1.0198949575424194]
                       Mean reward: -2.89
               Mean episode length: 124.00
                             Dones: 0.00
 Mean episode rew_tracking_lin_vel: 0.7849
 Mean episode rew_tracking_ang_vel: 0.0954
        Mean episode rew_lin_vel_z: -0.2603
       Mean episode rew_ang_vel_xy: -0.0233
          Mean episode rew_torques: -0.1355
          Mean episode rew_dof_acc: -0.0051
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -1.0067
      Mean episode rew_action_rate: -0.2294
   Mean episode rew_dof_pos_limits: -0.0010
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 600
                    Iteration time: 1.10s
                        Total time: 17.30s
                               ETA: 1153358.0s
################################################################################
                    [1m Learning iteration 15/1000000 
                       Computation: 35 steps/s (collection: 1.025s, learning 0.090s)
               Value function loss: 13056.3337
                    Surrogate loss: 41.4577
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 4.1900
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.96
     action noise std distribution: [0.8258612751960754, 1.0238487720489502, 1.0333607196807861, 0.8272918462753296, 1.015684962272644, 1.01670503616333, 0.8373938798904419, 1.0195438861846924, 1.0413331985473633, 0.8193310499191284, 1.0161769390106201, 1.0221445560455322]
                       Mean reward: -2.89
               Mean episode length: 124.00
                             Dones: 0.00
 Mean episode rew_tracking_lin_vel: 0.7849
 Mean episode rew_tracking_ang_vel: 0.0954
        Mean episode rew_lin_vel_z: -0.2603
       Mean episode rew_ang_vel_xy: -0.0233
          Mean episode rew_torques: -0.1355
          Mean episode rew_dof_acc: -0.0051
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -1.0067
      Mean episode rew_action_rate: -0.2294
   Mean episode rew_dof_pos_limits: -0.0010
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 640
                    Iteration time: 1.11s
                        Total time: 18.42s
                               ETA: 1150931.7s
################################################################################
                    [1m Learning iteration 16/1000000 
                       Computation: 34 steps/s (collection: 1.054s, learning 0.089s)
               Value function loss: 13131.6563
                    Surrogate loss: 20.3758
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.3499
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.96
     action noise std distribution: [0.8280568718910217, 1.0248945951461792, 1.036314606666565, 0.8290529251098633, 1.0166466236114502, 1.0177578926086426, 0.8391321897506714, 1.0221129655838013, 1.042478322982788, 0.8204365372657776, 1.0181152820587158, 1.0237985849380493]
                       Mean reward: -3.70
               Mean episode length: 129.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0785
 Mean episode rew_tracking_ang_vel: 0.2297
        Mean episode rew_lin_vel_z: -0.3464
       Mean episode rew_ang_vel_xy: -0.0319
          Mean episode rew_torques: -0.2173
          Mean episode rew_dof_acc: -0.0057
    Mean episode rew_feet_air_time: -0.0283
        Mean episode rew_collision: -1.6307
      Mean episode rew_action_rate: -0.2102
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 680
                    Iteration time: 1.14s
                        Total time: 19.56s
                               ETA: 1150471.0s
################################################################################
                    [1m Learning iteration 17/1000000 
                       Computation: 35 steps/s (collection: 1.035s, learning 0.089s)
               Value function loss: 21786.5584
                    Surrogate loss: 78.2869
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.2079
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.96
     action noise std distribution: [0.8292168974876404, 1.0244814157485962, 1.0391348600387573, 0.8318408727645874, 1.018777847290039, 1.0182377099990845, 0.8398527503013611, 1.0252981185913086, 1.0447033643722534, 0.8227749466896057, 1.020415186882019, 1.0259114503860474]
                       Mean reward: -3.70
               Mean episode length: 129.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.2446
        Mean episode rew_lin_vel_z: -0.3560
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_torques: -0.2264
          Mean episode rew_dof_acc: -0.0057
    Mean episode rew_feet_air_time: -0.0315
        Mean episode rew_collision: -1.7000
      Mean episode rew_action_rate: -0.2081
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720
                    Iteration time: 1.12s
                        Total time: 20.68s
                               ETA: 1149002.6s
################################################################################
                    [1m Learning iteration 18/1000000 
                       Computation: 33 steps/s (collection: 1.096s, learning 0.091s)
               Value function loss: 31625.8163
                    Surrogate loss: 318.3637
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.2047
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.96
     action noise std distribution: [0.8289523124694824, 1.0247266292572021, 1.0410165786743164, 0.8353496193885803, 1.0210379362106323, 1.0184825658798218, 0.84125155210495, 1.028955340385437, 1.0467288494110107, 0.8266595005989075, 1.0222821235656738, 1.0273751020431519]
                       Mean reward: -3.70
               Mean episode length: 129.00
                             Dones: 0.00
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.2446
        Mean episode rew_lin_vel_z: -0.3560
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_torques: -0.2264
          Mean episode rew_dof_acc: -0.0057
    Mean episode rew_feet_air_time: -0.0315
        Mean episode rew_collision: -1.7000
      Mean episode rew_action_rate: -0.2081
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 760
                    Iteration time: 1.19s
                        Total time: 21.87s
                               ETA: 1150994.2s
[91m[1m2025-03-20 16:38:37 [44,565ms] [Error] [omni.physx.plugin] Subscribtion cannot be changed during the event call.
################################################################################
                    [1m Learning iteration 19/1000000 
                       Computation: 36 steps/s (collection: 1.001s, learning 0.090s)
               Value function loss: 28726.1479
                    Surrogate loss: 200.8874
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.1702
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.97
     action noise std distribution: [0.83123779296875, 1.0273653268814087, 1.0439872741699219, 0.8388203382492065, 1.0237398147583008, 1.0192242860794067, 0.8453668355941772, 1.0307999849319458, 1.048944115638733, 0.8316710591316223, 1.0223634243011475, 1.0304882526397705]
                       Mean reward: -4.32
               Mean episode length: 132.33
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0317
 Mean episode rew_tracking_ang_vel: 0.2313
        Mean episode rew_lin_vel_z: -0.3543
       Mean episode rew_ang_vel_xy: -0.0326
          Mean episode rew_torques: -0.2277
          Mean episode rew_dof_acc: -0.0056
    Mean episode rew_feet_air_time: -0.0358
        Mean episode rew_collision: -1.6883
      Mean episode rew_action_rate: -0.2109
   Mean episode rew_dof_pos_limits: -0.0502
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 800
                    Iteration time: 1.09s
                        Total time: 22.96s
                               ETA: 1148005.7s
################################################################################
                    [1m Learning iteration 20/1000000 
                       Computation: 34 steps/s (collection: 1.089s, learning 0.059s)
               Value function loss: 28726.1479
                    Surrogate loss: 200.8874
   History latent supervision loss: 2.9763
  Privileged info regularizer loss: 3.1702
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.97
     action noise std distribution: [0.83123779296875, 1.0273653268814087, 1.0439872741699219, 0.8388203382492065, 1.0237398147583008, 1.0192242860794067, 0.8453668355941772, 1.0307999849319458, 1.048944115638733, 0.8316710591316223, 1.0223634243011475, 1.0304882526397705]
                       Mean reward: -4.32
               Mean episode length: 132.33
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.1813
 Mean episode rew_tracking_ang_vel: 0.1690
        Mean episode rew_lin_vel_z: -0.3466
       Mean episode rew_ang_vel_xy: -0.0315
          Mean episode rew_torques: -0.2335
          Mean episode rew_dof_acc: -0.0051
    Mean episode rew_feet_air_time: -0.0560
        Mean episode rew_collision: -1.6333
      Mean episode rew_action_rate: -0.2242
   Mean episode rew_dof_pos_limits: -0.2870
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 840
                    Iteration time: 1.15s
                        Total time: 24.11s
