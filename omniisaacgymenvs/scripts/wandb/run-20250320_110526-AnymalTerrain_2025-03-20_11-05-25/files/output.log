Actor MLP: Actor(
  (priv_encoder): Identity()
  (history_encoder): StateHistoryEncoder(
    (activation_fn): ELU(alpha=1.0)
    (encoder): Sequential(
      (0): Linear(in_features=48, out_features=30, bias=True)
      (1): ELU(alpha=1.0)
    )
    (conv_layers): Sequential(
      (0): Conv1d(30, 20, kernel_size=(4,), stride=(2,))
      (1): ELU(alpha=1.0)
      (2): Conv1d(20, 10, kernel_size=(2,), stride=(1,))
      (3): ELU(alpha=1.0)
      (4): Flatten(start_dim=1, end_dim=-1)
    )
    (linear_output): Sequential(
      (0): Linear(in_features=30, out_features=28, bias=True)
      (1): ELU(alpha=1.0)
    )
  )
  (actor_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (actor_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=12, bias=True)
    (5): Tanh()
  )
)
Critic MLP: Critic(
  (critic_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (critic_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
ActorCritic                              12
â”œâ”€Actor: 1-1                             --
â”‚    â””â”€Identity: 2-1                     --
â”‚    â””â”€StateHistoryEncoder: 2-2          --
â”‚    â”‚    â””â”€ELU: 3-1                     --
â”‚    â”‚    â””â”€Sequential: 3-2              1,470
â”‚    â”‚    â””â”€Sequential: 3-3              2,830
â”‚    â”‚    â””â”€Sequential: 3-4              868
â”‚    â””â”€Sequential: 2-3                   --
â”‚    â”‚    â””â”€Linear: 3-5                  9,856
â”‚    â”‚    â””â”€ELU: 3-6                     --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-7                  16,512
â”‚    â”‚    â””â”€ELU: 3-8                     --
â”‚    â”‚    â””â”€Linear: 3-9                  16,512
â”‚    â”‚    â””â”€ELU: 3-10                    --
â”‚    â”‚    â””â”€Linear: 3-11                 1,548
â”‚    â”‚    â””â”€Tanh: 3-12                   --
â”œâ”€Critic: 1-2                            --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-13                 9,856
â”‚    â”‚    â””â”€ELU: 3-14                    --
â”‚    â””â”€Sequential: 2-6                   --
â”‚    â”‚    â””â”€Linear: 3-15                 16,512
â”‚    â”‚    â””â”€ELU: 3-16                    --
â”‚    â”‚    â””â”€Linear: 3-17                 16,512
â”‚    â”‚    â””â”€ELU: 3-18                    --
â”‚    â”‚    â””â”€Linear: 3-19                 129
=================================================================
Total params: 92,617
Trainable params: 92,617
Non-trainable params: 0
=================================================================
[2025-03-20 11:05:29] Running RL reset
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/runs
[34m[1mwandb[39m[22m: [33mWARNING[39m Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'
################################################################################
                     [1m Learning iteration 0/1000000 
                       Computation: 24 steps/s (collection: 1.464s, learning 0.170s)
               Value function loss: 0.0000
                    Surrogate loss: 0.0000
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 0.0000
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.93
     action noise std distribution: [0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40
                    Iteration time: 1.63s
                        Total time: 1.63s
                               ETA: 1633678.2s
################################################################################
                     [1m Learning iteration 1/1000000 
                       Computation: 24 steps/s (collection: 1.081s, learning 0.554s)
               Value function loss: 0.0011
                    Surrogate loss: 279.3060
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9068
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8039748668670654, 1.0039572715759277, 1.0035043954849243, 0.8035142421722412, 1.002951741218567, 1.0001903772354126, 0.8026559948921204, 1.0010614395141602, 1.0016841888427734, 0.8036910891532898, 1.0036269426345825, 0.9974463582038879]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80
                    Iteration time: 1.64s
                        Total time: 3.27s
                               ETA: 1634435.7s
################################################################################
                     [1m Learning iteration 2/1000000 
                       Computation: 34 steps/s (collection: 1.087s, learning 0.088s)
               Value function loss: 0.0004
                    Surrogate loss: 517.0427
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 3.1610
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8076338171958923, 1.0067628622055054, 1.007151484489441, 0.8067343831062317, 1.0051722526550293, 1.0022642612457275, 0.8066484928131104, 1.0014588832855225, 1.0020993947982788, 0.8064841628074646, 1.006976842880249, 0.9988256096839905]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0002
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120
                    Iteration time: 1.17s
                        Total time: 4.44s
                               ETA: 1481288.4s
################################################################################
                     [1m Learning iteration 3/1000000 
                       Computation: 34 steps/s (collection: 1.070s, learning 0.088s)
               Value function loss: 0.0003
                    Surrogate loss: 20.8363
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9468
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8087082505226135, 1.009939432144165, 1.0095521211624146, 0.81075119972229, 1.0077909231185913, 1.0024417638778687, 0.8089289665222168, 1.0014922618865967, 1.0021347999572754, 0.8069871068000793, 1.0094475746154785, 0.9995864629745483]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0002
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0003
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160
                    Iteration time: 1.16s
                        Total time: 5.60s
                               ETA: 1400449.0s
################################################################################
                     [1m Learning iteration 4/1000000 
                       Computation: 33 steps/s (collection: 1.089s, learning 0.122s)
               Value function loss: 0.0015
                    Surrogate loss: 113.5941
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.8759
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8101435303688049, 1.0130574703216553, 1.01271390914917, 0.8149462938308716, 1.010915994644165, 1.0033799409866333, 0.8089476227760315, 1.0012657642364502, 1.0032017230987549, 0.8071907758712769, 1.0120394229888916, 0.9993399977684021]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0003
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 200
                    Iteration time: 1.21s
                        Total time: 6.81s
                               ETA: 1362545.0s
################################################################################
                     [1m Learning iteration 5/1000000 
                       Computation: 31 steps/s (collection: 1.181s, learning 0.090s)
               Value function loss: 0.0003
                    Surrogate loss: 52.6333
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 3.0039
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8119704723358154, 1.0148359537124634, 1.015852928161621, 0.8180568218231201, 1.014528512954712, 1.0046305656433105, 0.8089768886566162, 1.0028091669082642, 1.0040199756622314, 0.8083391785621643, 1.0131568908691406, 1.001319169998169]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 240
                    Iteration time: 1.27s
                        Total time: 8.08s
                               ETA: 1347342.4s
################################################################################
                     [1m Learning iteration 6/1000000 
                       Computation: 35 steps/s (collection: 1.051s, learning 0.088s)
               Value function loss: 0.0002
                    Surrogate loss: 28.4537
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9601
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8147712349891663, 1.0161056518554688, 1.018278956413269, 0.8209548592567444, 1.0167731046676636, 1.0075660943984985, 0.8114844560623169, 1.0040003061294556, 1.0037566423416138, 0.8113524317741394, 1.0147716999053955, 1.0036377906799316]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 280
                    Iteration time: 1.14s
                        Total time: 9.22s
                               ETA: 1317570.6s
################################################################################
                     [1m Learning iteration 7/1000000 
                       Computation: 35 steps/s (collection: 1.041s, learning 0.088s)
               Value function loss: 0.0003
                    Surrogate loss: 21.3253
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9051
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8169810771942139, 1.0155041217803955, 1.018206000328064, 0.8237429261207581, 1.0201548337936401, 1.0098698139190674, 0.8137792944908142, 1.0048471689224243, 1.0033448934555054, 0.8128969073295593, 1.0169270038604736, 1.004337191581726]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0003
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 320
                    Iteration time: 1.13s
                        Total time: 10.35s
                               ETA: 1294001.0s
################################################################################
                     [1m Learning iteration 8/1000000 
                       Computation: 35 steps/s (collection: 1.024s, learning 0.091s)
               Value function loss: 0.0002
                    Surrogate loss: 5.9131
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9510
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8185538053512573, 1.0156358480453491, 1.0180790424346924, 0.8248323798179626, 1.022249460220337, 1.0115019083023071, 0.8150900602340698, 1.007488489151001, 1.0030510425567627, 0.8145202994346619, 1.0195372104644775, 1.0058741569519043]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360
                    Iteration time: 1.12s
                        Total time: 11.47s
                               ETA: 1274181.2s
################################################################################
                     [1m Learning iteration 9/1000000 
                       Computation: 34 steps/s (collection: 1.075s, learning 0.088s)
               Value function loss: 0.0023
                    Surrogate loss: 286.0986
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9881
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8206886649131775, 1.017468810081482, 1.0201139450073242, 0.8254656195640564, 1.0229967832565308, 1.0137848854064941, 0.8159454464912415, 1.0095783472061157, 1.0035779476165771, 0.8158469796180725, 1.021034836769104, 1.0074166059494019]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 400
                    Iteration time: 1.16s
                        Total time: 12.63s
                               ETA: 1263069.2s
################################################################################
                    [1m Learning iteration 10/1000000 
                       Computation: 34 steps/s (collection: 1.075s, learning 0.089s)
               Value function loss: 0.0003
                    Surrogate loss: 7.5708
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9243
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8225449919700623, 1.0204302072525024, 1.021704912185669, 0.8261996507644653, 1.023956060409546, 1.0154869556427002, 0.8175875544548035, 1.0115877389907837, 1.0015531778335571, 0.8183676600456238, 1.0222342014312744, 1.0098689794540405]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 440
                    Iteration time: 1.16s
                        Total time: 13.79s
                               ETA: 1254049.4s
################################################################################
                    [1m Learning iteration 11/1000000 
                       Computation: 34 steps/s (collection: 1.055s, learning 0.092s)
               Value function loss: 0.0001
                    Surrogate loss: 34.5898
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 3.0763
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8246021270751953, 1.0216752290725708, 1.0230903625488281, 0.8277201056480408, 1.027122139930725, 1.0173990726470947, 0.8176150918006897, 1.0111974477767944, 1.0015738010406494, 0.820106565952301, 1.023846983909607, 1.012730360031128]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 480
                    Iteration time: 1.15s
                        Total time: 14.94s
                               ETA: 1245076.9s
################################################################################
                    [1m Learning iteration 12/1000000 
                       Computation: 34 steps/s (collection: 1.055s, learning 0.088s)
               Value function loss: 0.0009
                    Surrogate loss: 5.3864
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.8573
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8267329335212708, 1.0214072465896606, 1.0260869264602661, 0.8278995156288147, 1.0298351049423218, 1.0172524452209473, 0.8188421726226807, 1.0113329887390137, 1.0017979145050049, 0.8216740489006042, 1.0260119438171387, 1.0137630701065063]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 520
                    Iteration time: 1.14s
                        Total time: 16.08s
                               ETA: 1237235.2s
################################################################################
                    [1m Learning iteration 13/1000000 
                       Computation: 34 steps/s (collection: 1.074s, learning 0.096s)
               Value function loss: 0.0002
                    Surrogate loss: 11.2505
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.8602
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8295055627822876, 1.0219616889953613, 1.029721736907959, 0.8288437128067017, 1.0316499471664429, 1.016241431236267, 0.8214626312255859, 1.0129836797714233, 1.001421570777893, 0.8231295943260193, 1.02472984790802, 1.0146336555480957]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0001
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0003
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 560
                    Iteration time: 1.17s
                        Total time: 17.25s
                               ETA: 1232402.4s
################################################################################
                    [1m Learning iteration 14/1000000 
                       Computation: 33 steps/s (collection: 1.071s, learning 0.126s)
               Value function loss: 0.0002
                    Surrogate loss: 519.9189
   History latent supervision loss: 3.0366
  Privileged info regularizer loss: 2.9272
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.96
     action noise std distribution: [0.8312010765075684, 1.0239335298538208, 1.0273845195770264, 0.8312621712684631, 1.0318173170089722, 1.0179554224014282, 0.8250963687896729, 1.015416145324707, 1.0021146535873413, 0.8260982036590576, 1.025653600692749, 1.016600251197815]
                       Mean reward: -0.01
               Mean episode length: 1.00
                             Dones: 1.00
 Mean episode rew_tracking_lin_vel: 0.0002
 Mean episode rew_tracking_ang_vel: 0.0001
        Mean episode rew_lin_vel_z: -0.0002
       Mean episode rew_ang_vel_xy: -0.0000
          Mean episode rew_torques: -0.0002
          Mean episode rew_dof_acc: -0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0003
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 600
                    Iteration time: 1.20s
                        Total time: 18.45s
