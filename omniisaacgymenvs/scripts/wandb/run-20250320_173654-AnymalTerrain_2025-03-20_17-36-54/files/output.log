
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/runs
Actor MLP: Actor(
  (priv_encoder): Identity()
  (history_encoder): StateHistoryEncoder(
    (activation_fn): ELU(alpha=1.0)
    (encoder): Sequential(
      (0): Linear(in_features=48, out_features=30, bias=True)
      (1): ELU(alpha=1.0)
    )
    (conv_layers): Sequential(
      (0): Conv1d(30, 20, kernel_size=(4,), stride=(2,))
      (1): ELU(alpha=1.0)
      (2): Conv1d(20, 10, kernel_size=(2,), stride=(1,))
      (3): ELU(alpha=1.0)
      (4): Flatten(start_dim=1, end_dim=-1)
    )
    (linear_output): Sequential(
      (0): Linear(in_features=30, out_features=28, bias=True)
      (1): ELU(alpha=1.0)
    )
  )
  (actor_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (actor_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=12, bias=True)
    (5): Tanh()
  )
)
Critic MLP: Critic(
  (critic_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (critic_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
ActorCritic                              12
â”œâ”€Actor: 1-1                             --
â”‚    â””â”€Identity: 2-1                     --
â”‚    â””â”€StateHistoryEncoder: 2-2          --
â”‚    â”‚    â””â”€ELU: 3-1                     --
â”‚    â”‚    â””â”€Sequential: 3-2              1,470
â”‚    â”‚    â””â”€Sequential: 3-3              2,830
â”‚    â”‚    â””â”€Sequential: 3-4              868
â”‚    â””â”€Sequential: 2-3                   --
â”‚    â”‚    â””â”€Linear: 3-5                  9,856
â”‚    â”‚    â””â”€ELU: 3-6                     --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-7                  16,512
â”‚    â”‚    â””â”€ELU: 3-8                     --
â”‚    â”‚    â””â”€Linear: 3-9                  16,512
â”‚    â”‚    â””â”€ELU: 3-10                    --
â”‚    â”‚    â””â”€Linear: 3-11                 1,548
â”‚    â”‚    â””â”€Tanh: 3-12                   --
â”œâ”€Critic: 1-2                            --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-13                 9,856
â”‚    â”‚    â””â”€ELU: 3-14                    --
â”‚    â””â”€Sequential: 2-6                   --
â”‚    â”‚    â””â”€Linear: 3-15                 16,512
â”‚    â”‚    â””â”€ELU: 3-16                    --
â”‚    â”‚    â””â”€Linear: 3-17                 16,512
â”‚    â”‚    â””â”€ELU: 3-18                    --
â”‚    â”‚    â””â”€Linear: 3-19                 129
=================================================================
Total params: 92,617
Trainable params: 92,617
Non-trainable params: 0
=================================================================
[2025-03-20 17:36:57] Running RL reset
[34m[1mwandb[39m[22m: [33mWARNING[39m Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'
################################################################################
                     [1m Learning iteration 0/1000000 
                       Computation: 27 steps/s (collection: 1.307s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: 0.0000
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.93
     action noise std distribution: [0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40
                    Iteration time: 1.47s
                        Total time: 1.47s
                               ETA: 1472248.8s
################################################################################
                     [1m Learning iteration 1/1000000 
                       Computation: 36 steps/s (collection: 1.000s, learning 0.100s)
               Value function loss: 0.0023
                    Surrogate loss: 14.1810
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7993208765983582, 1.0013532638549805, 1.00283944606781, 0.803728461265564, 1.0020198822021484, 1.0026124715805054, 0.803173303604126, 1.002321720123291, 1.002862811088562, 0.8020820617675781, 1.001436710357666, 0.9998897314071655]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80
                    Iteration time: 1.10s
                        Total time: 2.57s
                               ETA: 1286021.1s
################################################################################
                     [1m Learning iteration 2/1000000 
                       Computation: 36 steps/s (collection: 1.016s, learning 0.090s)
               Value function loss: 0.0030
                    Surrogate loss: 38.6255
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7983406782150269, 1.0000416040420532, 1.0053927898406982, 0.8076226711273193, 1.0031315088272095, 1.0059878826141357, 0.8049148917198181, 1.0012743473052979, 1.0044444799423218, 0.8042357563972473, 1.0041382312774658, 1.001532793045044]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120
                    Iteration time: 1.11s
                        Total time: 3.68s
                               ETA: 1225937.9s
################################################################################
                     [1m Learning iteration 3/1000000 
                       Computation: 37 steps/s (collection: 0.979s, learning 0.089s)
               Value function loss: 0.1803
                    Surrogate loss: 30.6919
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4227
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7995023131370544, 1.0004273653030396, 1.0052549839019775, 0.8115960359573364, 1.0020369291305542, 1.006239891052246, 0.8064581751823425, 1.0018284320831299, 1.005419373512268, 0.8033871054649353, 1.0071371793746948, 1.0018624067306519]
                       Mean reward: -0.98
               Mean episode length: 148.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0388
 Mean episode rew_tracking_ang_vel: 0.0111
        Mean episode rew_lin_vel_z: -0.0305
       Mean episode rew_ang_vel_xy: -0.0043
          Mean episode rew_torques: -0.0474
          Mean episode rew_dof_acc: -0.0008
    Mean episode rew_feet_air_time: -0.0071
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0656
   Mean episode rew_dof_pos_limits: -0.0007
        Mean episode terrain_level: 0.3250
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160
                    Iteration time: 1.07s
                        Total time: 4.75s
                               ETA: 1186527.1s
################################################################################
                     [1m Learning iteration 4/1000000 
                       Computation: 35 steps/s (collection: 1.041s, learning 0.086s)
               Value function loss: 2.1294
                    Surrogate loss: 247.0393
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.2947
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8008855581283569, 1.0027872323989868, 1.0061978101730347, 0.8162765502929688, 1.0032563209533691, 1.0082123279571533, 0.810754656791687, 1.002988338470459, 1.008800983428955, 0.8045550584793091, 1.0106134414672852, 1.0031427145004272]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.02
 Mean episode rew_tracking_lin_vel: 0.1312
 Mean episode rew_tracking_ang_vel: 0.0394
        Mean episode rew_lin_vel_z: -0.1108
       Mean episode rew_ang_vel_xy: -0.0141
          Mean episode rew_torques: -0.1373
          Mean episode rew_dof_acc: -0.0028
    Mean episode rew_feet_air_time: -0.0202
        Mean episode rew_collision: -0.0330
      Mean episode rew_action_rate: -0.1918
   Mean episode rew_dof_pos_limits: -0.0020
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 200
                    Iteration time: 1.13s
                        Total time: 5.87s
                               ETA: 1174587.8s
################################################################################
                     [1m Learning iteration 5/1000000 
                       Computation: 37 steps/s (collection: 0.992s, learning 0.088s)
               Value function loss: 0.2152
                    Surrogate loss: 199.6859
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.3989
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8024036884307861, 1.003431797027588, 1.0084542036056519, 0.8169198632240295, 1.0052894353866577, 1.0107721090316772, 0.8134571313858032, 1.004378080368042, 1.0127204656600952, 0.8043176531791687, 1.011459231376648, 1.0042500495910645]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.02
 Mean episode rew_tracking_lin_vel: 0.2761
 Mean episode rew_tracking_ang_vel: 0.1039
        Mean episode rew_lin_vel_z: -0.3212
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_torques: -0.0323
          Mean episode rew_dof_acc: -0.0053
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -0.4400
      Mean episode rew_action_rate: -0.0690
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 240
                    Iteration time: 1.08s
                        Total time: 6.95s
                               ETA: 1158934.7s
################################################################################
                     [1m Learning iteration 6/1000000 
                       Computation: 35 steps/s (collection: 1.046s, learning 0.087s)
               Value function loss: 0.0922
                    Surrogate loss: 35.3256
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.4193
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.8053492307662964, 1.0039246082305908, 1.0106996297836304, 0.818119466304779, 1.006569743156433, 1.0104516744613647, 0.8162829875946045, 1.0069117546081543, 1.0167291164398193, 0.8037411570549011, 1.0134575366973877, 1.0053393840789795]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.2761
 Mean episode rew_tracking_ang_vel: 0.1039
        Mean episode rew_lin_vel_z: -0.3212
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_torques: -0.0323
          Mean episode rew_dof_acc: -0.0053
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -0.4400
      Mean episode rew_action_rate: -0.0690
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 280
                    Iteration time: 1.13s
                        Total time: 8.09s
                               ETA: 1155238.1s
################################################################################
                     [1m Learning iteration 7/1000000 
                       Computation: 35 steps/s (collection: 1.048s, learning 0.087s)
               Value function loss: 0.1045
                    Surrogate loss: 156.8579
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.4369
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8092195987701416, 1.0053595304489136, 1.013448715209961, 0.8203052282333374, 1.0082991123199463, 1.0112534761428833, 0.82017582654953, 1.008950114250183, 1.0207247734069824, 0.8053151965141296, 1.0164223909378052, 1.0078262090682983]
                       Mean reward: -1.26
               Mean episode length: 99.00
                             Dones: 0.00
 Mean episode rew_tracking_lin_vel: 0.2761
 Mean episode rew_tracking_ang_vel: 0.1039
        Mean episode rew_lin_vel_z: -0.3212
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_torques: -0.0323
          Mean episode rew_dof_acc: -0.0053
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: -0.4400
      Mean episode rew_action_rate: -0.0690
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 320
                    Iteration time: 1.13s
                        Total time: 9.22s
                               ETA: 1152682.3s
################################################################################
                     [1m Learning iteration 8/1000000 
                       Computation: 35 steps/s (collection: 1.033s, learning 0.089s)
               Value function loss: 13.4971
                    Surrogate loss: 14.7906
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.1211
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8118024468421936, 1.0077800750732422, 1.0154322385787964, 0.8207824230194092, 1.0101181268692017, 1.0132231712341309, 0.8227232694625854, 1.009847640991211, 1.023617148399353, 0.8080551624298096, 1.0158324241638184, 1.0075047016143799]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.1798
 Mean episode rew_tracking_ang_vel: 0.1303
        Mean episode rew_lin_vel_z: -0.2914
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_torques: -0.0733
          Mean episode rew_dof_acc: -0.0046
    Mean episode rew_feet_air_time: -0.0030
        Mean episode rew_collision: -0.9090
      Mean episode rew_action_rate: -0.1196
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360
                    Iteration time: 1.12s
                        Total time: 10.34s
                               ETA: 1149269.4s
################################################################################
                     [1m Learning iteration 9/1000000 
                       Computation: 38 steps/s (collection: 0.948s, learning 0.089s)
               Value function loss: 190.2744
                    Surrogate loss: 159.4751
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4514
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8144801259040833, 1.0112762451171875, 1.0186890363693237, 0.8222103714942932, 1.0128408670425415, 1.014978289604187, 0.8262513875961304, 1.0117356777191162, 1.0261675119400024, 0.811477780342102, 1.0158309936523438, 1.0090667009353638]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0009
 Mean episode rew_tracking_ang_vel: 0.1792
        Mean episode rew_lin_vel_z: -0.2360
       Mean episode rew_ang_vel_xy: -0.0075
          Mean episode rew_torques: -0.1495
          Mean episode rew_dof_acc: -0.0033
    Mean episode rew_feet_air_time: -0.0085
        Mean episode rew_collision: -1.7800
      Mean episode rew_action_rate: -0.2136
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 400
                    Iteration time: 1.04s
                        Total time: 11.38s
                               ETA: 1138029.6s
################################################################################
                    [1m Learning iteration 10/1000000 
                       Computation: 37 steps/s (collection: 0.971s, learning 0.087s)
               Value function loss: 437.9857
                    Surrogate loss: 106.9117
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4632
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8176791667938232, 1.0135585069656372, 1.0224236249923706, 0.8228294849395752, 1.0144615173339844, 1.0141890048980713, 0.8294875621795654, 1.013823390007019, 1.0291317701339722, 0.8145076036453247, 1.0168325901031494, 1.0123320817947388]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0009
 Mean episode rew_tracking_ang_vel: 0.1792
        Mean episode rew_lin_vel_z: -0.2360
       Mean episode rew_ang_vel_xy: -0.0075
          Mean episode rew_torques: -0.1495
          Mean episode rew_dof_acc: -0.0033
    Mean episode rew_feet_air_time: -0.0085
        Mean episode rew_collision: -1.7800
      Mean episode rew_action_rate: -0.2136
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 440
                    Iteration time: 1.06s
                        Total time: 12.44s
                               ETA: 1130752.8s
################################################################################
                    [1m Learning iteration 11/1000000 
                       Computation: 36 steps/s (collection: 0.991s, learning 0.090s)
               Value function loss: 1146.4254
                    Surrogate loss: 9.1796
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4666
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8209311962127686, 1.0156632661819458, 1.0242938995361328, 0.8233876824378967, 1.0144137144088745, 1.0143771171569824, 0.8322690725326538, 1.0162923336029053, 1.0321930646896362, 0.8160780072212219, 1.016311526298523, 1.0152194499969482]
                       Mean reward: -3.07
               Mean episode length: 115.67
                             Dones: 0.00
 Mean episode rew_tracking_lin_vel: 0.0009
 Mean episode rew_tracking_ang_vel: 0.1792
        Mean episode rew_lin_vel_z: -0.2360
       Mean episode rew_ang_vel_xy: -0.0075
          Mean episode rew_torques: -0.1495
          Mean episode rew_dof_acc: -0.0033
    Mean episode rew_feet_air_time: -0.0085
        Mean episode rew_collision: -1.7800
      Mean episode rew_action_rate: -0.2136
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 480
                    Iteration time: 1.08s
                        Total time: 13.52s
                               ETA: 1126629.3s
################################################################################
                    [1m Learning iteration 12/1000000 
                       Computation: 36 steps/s (collection: 1.006s, learning 0.088s)
               Value function loss: 1304.9025
                    Surrogate loss: 49.4217
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 3.4728
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.95
     action noise std distribution: [0.8228293657302856, 1.017072319984436, 1.0259969234466553, 0.8240023255348206, 1.014121651649475, 1.0146476030349731, 0.8328494429588318, 1.0174630880355835, 1.0343356132507324, 0.8168600797653198, 1.0153089761734009, 1.0166574716567993]
                       Mean reward: -2.89
               Mean episode length: 124.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.4909
 Mean episode rew_tracking_ang_vel: 0.1269
        Mean episode rew_lin_vel_z: -0.2512
       Mean episode rew_ang_vel_xy: -0.0173
          Mean episode rew_torques: -0.1407
          Mean episode rew_dof_acc: -0.0044
    Mean episode rew_feet_air_time: -0.0032
        Mean episode rew_collision: -1.2967
      Mean episode rew_action_rate: -0.2234
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode terrain_level: 1.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 520
                    Iteration time: 1.09s
                        Total time: 14.61s
