Actor MLP: Actor(
  (priv_encoder): Identity()
  (history_encoder): StateHistoryEncoder(
    (activation_fn): ELU(alpha=1.0)
    (encoder): Sequential(
      (0): Linear(in_features=48, out_features=30, bias=True)
      (1): ELU(alpha=1.0)
    )
    (conv_layers): Sequential(
      (0): Conv1d(30, 20, kernel_size=(4,), stride=(2,))
      (1): ELU(alpha=1.0)
      (2): Conv1d(20, 10, kernel_size=(2,), stride=(1,))
      (3): ELU(alpha=1.0)
      (4): Flatten(start_dim=1, end_dim=-1)
    )
    (linear_output): Sequential(
      (0): Linear(in_features=30, out_features=28, bias=True)
      (1): ELU(alpha=1.0)
    )
  )
  (actor_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (actor_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=12, bias=True)
    (5): Tanh()
  )
)
Critic MLP: Critic(
  (critic_backbone): Sequential(
    (0): Linear(in_features=76, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
  )
  (critic_leg_control_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
ActorCritic                              12
â”œâ”€Actor: 1-1                             --
â”‚    â””â”€Identity: 2-1                     --
â”‚    â””â”€StateHistoryEncoder: 2-2          --
â”‚    â”‚    â””â”€ELU: 3-1                     --
â”‚    â”‚    â””â”€Sequential: 3-2              1,470
â”‚    â”‚    â””â”€Sequential: 3-3              2,830
â”‚    â”‚    â””â”€Sequential: 3-4              868
â”‚    â””â”€Sequential: 2-3                   --
â”‚    â”‚    â””â”€Linear: 3-5                  9,856
â”‚    â”‚    â””â”€ELU: 3-6                     --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-7                  16,512
â”‚    â”‚    â””â”€ELU: 3-8                     --
â”‚    â”‚    â””â”€Linear: 3-9                  16,512
â”‚    â”‚    â””â”€ELU: 3-10                    --
â”‚    â”‚    â””â”€Linear: 3-11                 1,548
â”‚    â”‚    â””â”€Tanh: 3-12                   --
â”œâ”€Critic: 1-2                            --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-13                 9,856
â”‚    â”‚    â””â”€ELU: 3-14                    --
â”‚    â””â”€Sequential: 2-6                   --
â”‚    â”‚    â””â”€Linear: 3-15                 16,512
â”‚    â”‚    â””â”€ELU: 3-16                    --
â”‚    â”‚    â””â”€Linear: 3-17                 16,512
â”‚    â”‚    â””â”€ELU: 3-18                    --
â”‚    â”‚    â””â”€Linear: 3-19                 129
=================================================================
Total params: 92,617
Trainable params: 92,617
Non-trainable params: 0
=================================================================
[2025-03-20 17:33:40] Running RL reset
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/runs
################################################################################
                     [1m Learning iteration 0/1000000 
                       Computation: 26 steps/s (collection: 1.362s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: 0.0000
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.93
     action noise std distribution: [0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0, 0.800000011920929, 1.0, 1.0]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40
                    Iteration time: 1.52s
                        Total time: 1.52s
                               ETA: 1523545.0s
################################################################################
                     [1m Learning iteration 1/1000000 
                       Computation: 35 steps/s (collection: 1.029s, learning 0.103s)
               Value function loss: 0.0023
                    Surrogate loss: 14.1810
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7993208765983582, 1.0013532638549805, 1.00283944606781, 0.803728461265564, 1.0020198822021484, 1.0026124715805054, 0.803173303604126, 1.002321720123291, 1.002862811088562, 0.8020820617675781, 1.001436710357666, 0.9998897314071655]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80
                    Iteration time: 1.13s
                        Total time: 2.66s
                               ETA: 1327592.2s
[34m[1mwandb[39m[22m: [33mWARNING[39m Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'
################################################################################
                     [1m Learning iteration 2/1000000 
                       Computation: 38 steps/s (collection: 0.949s, learning 0.088s)
               Value function loss: 0.0030
                    Surrogate loss: 38.6255
   History latent supervision loss: 2.5880
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7983406782150269, 1.0000416040420532, 1.0053927898406982, 0.8076226711273193, 1.0031315088272095, 1.0059878826141357, 0.8049148917198181, 1.0012743473052979, 1.0044444799423218, 0.8042357563972473, 1.0041382312774658, 1.001532793045044]
 Mean episode rew_tracking_lin_vel: 0.0000
 Mean episode rew_tracking_ang_vel: 0.0000
        Mean episode rew_lin_vel_z: 0.0000
       Mean episode rew_ang_vel_xy: 0.0000
          Mean episode rew_torques: 0.0000
          Mean episode rew_dof_acc: 0.0000
    Mean episode rew_feet_air_time: 0.0000
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: 0.0000
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode terrain_level: 0.0000
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120
                    Iteration time: 1.04s
                        Total time: 3.69s
                               ETA: 1230739.8s
Error executing job with overrides: []
################################################################################
                     [1m Learning iteration 3/1000000 
                       Computation: 37 steps/s (collection: 0.976s, learning 0.092s)
               Value function loss: 0.1803
                    Surrogate loss: 30.6919
   History latent supervision loss: 2.5880
  Privileged info regularizer loss: 2.4227
Privileged info regularizer lambda: 0.0000
         Leg mean action noise std: 0.94
     action noise std distribution: [0.7995023131370544, 1.0004273653030396, 1.0052549839019775, 0.8115960359573364, 1.0020369291305542, 1.006239891052246, 0.8064581751823425, 1.0018284320831299, 1.005419373512268, 0.8033871054649353, 1.0071371793746948, 1.0018624067306519]
                       Mean reward: -0.98
               Mean episode length: 148.00
                             Dones: 0.01
 Mean episode rew_tracking_lin_vel: 0.0388
 Mean episode rew_tracking_ang_vel: 0.0111
        Mean episode rew_lin_vel_z: -0.0305
       Mean episode rew_ang_vel_xy: -0.0043
          Mean episode rew_torques: -0.0474
          Mean episode rew_dof_acc: -0.0008
    Mean episode rew_feet_air_time: -0.0071
        Mean episode rew_collision: 0.0000
      Mean episode rew_action_rate: -0.0656
   Mean episode rew_dof_pos_limits: -0.0007
        Mean episode terrain_level: 0.3250
        Mean episode max_command_x: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160
                    Iteration time: 1.07s
                        Total time: 4.76s
                               ETA: 1190115.1s
Traceback (most recent call last):
  File "/media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/scripts/train.py", line 175, in parse_hydra_configs
    rlg_trainer.run(env, module_path, experiment_dir)
  File "/media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/scripts/train.py", line 89, in run
    runner.learn(num_learning_iterations=1000000, init_at_random_ep_len=True)
  File "/media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/rsl_rl/rsl_rl/runners/on_policy_runner.py", line 130, in learn
    obs, privileged_obs, rewards, dones, infos = self.env.step(actions.to(self.device))
  File "/media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/envs/vec_env_rlgames.py", line 108, in step
    self._obs, self._priv_obs, self._rew, self._resets, self._extras = self._task.post_physics_step()
  File "/media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/tasks/anymal_terrain.py", line 1010, in post_physics_step
    self.reset_idx(env_ids)
  File "/media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/tasks/anymal_terrain.py", line 841, in reset_idx
    self.update_terrain_level(env_ids)
  File "/media/isaac/Daten/azhar_ws/leggedOmniIsaacGymEnvs/OmniIsaacGymEnvs/omniisaacgymenvs/tasks/anymal_terrain.py", line 922, in update_terrain_level
    candidate_indices = self._terrains_by_level[lvl.item()]   # All detail rows matching 'lvl'
KeyError: 2
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.