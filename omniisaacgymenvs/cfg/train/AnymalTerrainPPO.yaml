params:
  config:
    name: ${resolve_default:AnymalTerrain,${....experiment}}
    device: ${....rl_device}

policy:
  # Only include noise for leg actions (num_leg_actions = 12)
  # Using the first 12 values of the original init_std specification
  init_std: [0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0]
  actor_hidden_dims: [128]
  critic_hidden_dims: [128]
  activation: elu
  # Only leg-specific control head remains.
  leg_control_head_hidden_dims: [128, 128]
  priv_encoder_dims: [64, 20]
  num_leg_actions: 12

algorithm:
  value_loss_coef: 1.0
  use_clipped_value_loss: true
  clip_param: 0.2
  entropy_coef: 0.0
  num_learning_epochs: 5
  num_mini_batches: 4
  learning_rate: 2e-4
  schedule: fixed
  gamma: 0.99
  lam: 0.95
  desired_kl: null
  max_grad_norm: 1.0
  # Update min_policy_std to match the leg-only action space (12 values)
  min_policy_std: [0.15, 0.25, 0.25, 0.15, 0.25, 0.25, 0.15, 0.25, 0.25, 0.15, 0.25, 0.25]
  dagger_update_freq: 20
  priv_reg_coef_schedual: [0, 0, 3000, 7000]

runner:
  policy_class_name: ActorCritic
  algorithm_class_name: PPO
  num_steps_per_env: 40
  max_iterations: 40000
  save_interval: 500
  experiment_name: rough_widowGo1
  run_name: ""
  resume: false
  load_run: -1
  checkpoint: -1
  resume_path: null
