params:
  config:
    name: ${resolve_default:AnymalTerrain,${....experiment}}
    device: ${....rl_device}

policy:
  # Only include noise for leg actions (num_leg_actions = 12)
  # Using the first 12 values of the original init_std specification
  init_std: [0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0]
  actor_hidden_dims: [512]
  critic_hidden_dims: [512]
  activation: elu
  # Only leg-specific control head remains.
  leg_control_head_hidden_dims: [256, 128]
  priv_encoder_dims: [64, 32, 24] #[128,128] #64, 20
  scan_encoder_dims: [128, 64, 32] #[128, 64, 32] #[]
  num_leg_actions: 12

algorithm:
  value_loss_coef: 1.0
  use_clipped_value_loss: True
  clip_param: 0.2
  entropy_coef: 0.005 #0.01
  num_learning_epochs: 7
  num_mini_batches: 2
  learning_rate: 1.0e-3 #7.e-4 #1.0e-3 #1.0e-3 #3.e-4  #5.e-4
  schedule: 'adaptive' #fixed #'adaptive'
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.01 #0.008 #null #0.01
  bounds_loss_coef: 0.0
  max_grad_norm: 1.0
  # Update min_policy_std to match the action space (12 values)
  min_policy_std: [0.15, 0.25, 0.25, 0.15, 0.25, 0.25, 0.15, 0.25, 0.25, 0.15, 0.25, 0.25]
  use_history_encoding: True
  history_encoder_learning_rate: 1.0e-3
  dagger_update_freq: 20
  priv_reg_coef_schedual:
  - [0,   0.1, 0, 2000]
  - [0.1, 1.0, 2000, 1000]

runner:
  policy_class_name: ActorCritic
  algorithm_class_name: PPO
  num_steps_per_env: 65 #24
  max_iterations: 3000
  save_interval: 500
  curriculum_log_interval: 10

